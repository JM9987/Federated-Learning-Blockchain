{"ast":null,"code":"'use strict';\n\nconst log = require('debug')('ipfs:mfs:write');\n\nconst importer = require('ipfs-unixfs-importer');\n\nconst stat = require('./stat');\n\nconst mkdir = require('./mkdir');\n\nconst addLink = require('./utils/add-link');\n\nconst mergeOptions = require('merge-options').bind({\n  ignoreUndefined: true\n});\n\nconst createLock = require('./utils/create-lock');\n\nconst toAsyncIterator = require('./utils/to-async-iterator');\n\nconst toMfsPath = require('./utils/to-mfs-path');\n\nconst toPathComponents = require('./utils/to-path-components');\n\nconst toTrail = require('./utils/to-trail');\n\nconst updateTree = require('./utils/update-tree');\n\nconst updateMfsRoot = require('./utils/update-mfs-root');\n\nconst errCode = require('err-code');\n\nconst _require = require('../../utils'),\n      MFS_MAX_CHUNK_SIZE = _require.MFS_MAX_CHUNK_SIZE;\n\nconst last = require('it-last');\n\nconst withTimeoutOption = require('ipfs-core-utils/src/with-timeout-option');\n\nconst defaultOptions = {\n  offset: 0,\n  // the offset in the file to begin writing\n  length: undefined,\n  // how many bytes from the incoming buffer to write\n  create: false,\n  // whether to create the file if it does not exist\n  truncate: false,\n  // whether to truncate the file first\n  rawLeaves: false,\n  reduceSingleLeafToSelf: false,\n  cidVersion: 0,\n  hashAlg: 'sha2-256',\n  parents: false,\n  // whether to create intermediate directories if they do not exist\n  progress: () => {},\n  strategy: 'trickle',\n  flush: true,\n  leafType: 'raw',\n  shardSplitThreshold: 1000,\n  mode: undefined,\n  mtime: undefined,\n  signal: undefined\n};\n\nmodule.exports = context => {\n  /**\n   * Write to an MFS path\n   *\n   * @param {string} path - The MFS path where you will write to\n   * @param {string|Uint8Array|AsyncIterable<Uint8Array>|Blob} content - The content to write to the path\n   * @param {WriteOptions & AbortOptions} [options]\n   * @returns {Promise<void>}\n   */\n  async function mfsWrite(path, content, options = {}) {\n    options = mergeOptions(defaultOptions, options);\n    let source, destination, parent;\n    log('Reading source, destination and parent');\n    await createLock().readLock(async () => {\n      source = await toAsyncIterator(content);\n      destination = await toMfsPath(context, path, options);\n      parent = await toMfsPath(context, destination.mfsDirectory, options);\n    })();\n    log('Read source, destination and parent'); // @ts-ignore - parent maybe undefined\n\n    if (!options.parents && !parent.exists) {\n      throw errCode(new Error('directory does not exist'), 'ERR_NO_EXIST');\n    } // @ts-ignore - parent maybe undefined\n\n\n    if (!options.create && !destination.exists) {\n      throw errCode(new Error('file does not exist'), 'ERR_NO_EXIST');\n    }\n\n    return updateOrImport(context, path, source, destination, options);\n  }\n\n  return withTimeoutOption(mfsWrite);\n};\n\nconst updateOrImport = async (context, path, source, destination, options) => {\n  const child = await write(context, source, destination, options); // The slow bit is done, now add or replace the DAGLink in the containing directory\n  // re-reading the path to the containing folder in case it has changed in the interim\n\n  await createLock().writeLock(async () => {\n    const pathComponents = toPathComponents(path);\n    const fileName = pathComponents.pop();\n    let parentExists = false;\n\n    try {\n      await stat(context)(\"/\".concat(pathComponents.join('/')), options);\n      parentExists = true;\n    } catch (err) {\n      if (err.code !== 'ERR_NOT_FOUND') {\n        throw err;\n      }\n    }\n\n    if (!parentExists) {\n      await mkdir(context)(\"/\".concat(pathComponents.join('/')), options);\n    } // get an updated mfs path in case the root changed while we were writing\n\n\n    const updatedPath = await toMfsPath(context, path, options);\n    const trail = await toTrail(context, updatedPath.mfsDirectory);\n    const parent = trail[trail.length - 1];\n\n    if (!parent.type.includes('directory')) {\n      throw errCode(new Error(\"cannot write to \".concat(parent.name, \": Not a directory\")), 'ERR_NOT_A_DIRECTORY');\n    }\n\n    const parentNode = await context.ipld.get(parent.cid);\n    const result = await addLink(context, {\n      parent: parentNode,\n      name: fileName,\n      cid: child.cid,\n      size: child.size,\n      flush: options.flush,\n      shardSplitThreshold: options.shardSplitThreshold,\n      hashAlg: options.hashAlg,\n      cidVersion: options.cidVersion\n    });\n    parent.cid = result.cid; // update the tree with the new child\n\n    const newRootCid = await updateTree(context, trail, options); // Update the MFS record with the new CID for the root of the tree\n\n    await updateMfsRoot(context, newRootCid, options);\n  })();\n};\n\nconst write = async (context, source, destination, options) => {\n  if (destination.exists) {\n    log(\"Overwriting file \".concat(destination.cid, \" offset \").concat(options.offset, \" length \").concat(options.length));\n  } else {\n    log(\"Writing file offset \".concat(options.offset, \" length \").concat(options.length));\n  }\n\n  const sources = []; // pad start of file if necessary\n\n  if (options.offset > 0) {\n    if (destination.unixfs) {\n      log(\"Writing first \".concat(options.offset, \" bytes of original file\"));\n      sources.push(() => {\n        return destination.content({\n          offset: 0,\n          length: options.offset\n        });\n      });\n\n      if (destination.unixfs.fileSize() < options.offset) {\n        const extra = options.offset - destination.unixfs.fileSize();\n        log(\"Writing zeros for extra \".concat(extra, \" bytes\"));\n        sources.push(asyncZeroes(extra));\n      }\n    } else {\n      log(\"Writing zeros for first \".concat(options.offset, \" bytes\"));\n      sources.push(asyncZeroes(options.offset));\n    }\n  }\n\n  sources.push(limitAsyncStreamBytes(source, options.length));\n  const content = countBytesStreamed(catAsyncIterators(sources), bytesWritten => {\n    if (destination.unixfs && !options.truncate) {\n      // if we've done reading from the new source and we are not going\n      // to truncate the file, add the end of the existing file to the output\n      const fileSize = destination.unixfs.fileSize();\n\n      if (fileSize > bytesWritten) {\n        log(\"Writing last \".concat(fileSize - bytesWritten, \" of \").concat(fileSize, \" bytes from original file starting at offset \").concat(bytesWritten));\n        return destination.content({\n          offset: bytesWritten\n        });\n      } else {\n        log('Not writing last bytes from original file');\n      }\n    }\n\n    return {\n      [Symbol.asyncIterator]: async function* () {}\n    };\n  });\n  let mode;\n\n  if (options.mode !== undefined && options.mode !== null) {\n    mode = options.mode;\n  } else if (destination && destination.unixfs) {\n    mode = destination.unixfs.mode;\n  }\n\n  let mtime;\n\n  if (options.mtime !== undefined && options.mtine !== null) {\n    mtime = options.mtime;\n  } else if (destination && destination.unixfs) {\n    mtime = destination.unixfs.mtime;\n  }\n\n  const result = await last(importer([{\n    content: content,\n    // persist mode & mtime if set previously\n    mode,\n    mtime\n  }], context.block, {\n    progress: options.progress,\n    hashAlg: options.hashAlg,\n    cidVersion: options.cidVersion,\n    strategy: options.strategy,\n    rawLeaves: options.rawLeaves,\n    reduceSingleLeafToSelf: options.reduceSingleLeafToSelf,\n    leafType: options.leafType,\n    pin: false\n  }));\n  log(\"Wrote \".concat(result.cid));\n  return {\n    cid: result.cid,\n    size: result.size\n  };\n};\n\nconst limitAsyncStreamBytes = (stream, limit) => {\n  return async function* _limitAsyncStreamBytes() {\n    let emitted = 0;\n\n    for await (const buf of stream) {\n      emitted += buf.length;\n\n      if (emitted > limit) {\n        yield buf.slice(0, limit - emitted);\n        return;\n      }\n\n      yield buf;\n    }\n  };\n};\n\nconst asyncZeroes = (count, chunkSize = MFS_MAX_CHUNK_SIZE) => {\n  const buf = new Uint8Array(chunkSize);\n  const stream = {\n    [Symbol.asyncIterator]: function* _asyncZeroes() {\n      while (true) {\n        yield buf.slice();\n      }\n    }\n  };\n  return limitAsyncStreamBytes(stream, count);\n};\n\nconst catAsyncIterators = async function* (sources) {\n  // eslint-disable-line require-await\n  for (let i = 0; i < sources.length; i++) {\n    yield* sources[i]();\n  }\n};\n\nconst countBytesStreamed = async function* (source, notify) {\n  let wrote = 0;\n\n  for await (const buf of source) {\n    wrote += buf.length;\n    yield buf;\n  }\n\n  for await (const buf of notify(wrote)) {\n    wrote += buf.length;\n    yield buf;\n  }\n};\n/**\n * @typedef {Object} WriteOptions\n * @property {number} [offset] - An offset to start writing to file at\n * @property {number} [length] - Optionally limit how many bytes are read from the stream\n * @property {boolean} [create=false] - Create the MFS path if it does not exist\n * @property {boolean} [parents=false] - Create intermediate MFS paths if they do not exist\n * @property {boolean} [truncate=false] - Truncate the file at the MFS path if it would have been larger than the passed content\n * @property {boolean} [rawLeaves=false] - If true, DAG leaves will contain raw file data and not be wrapped in a protobuf\n * @property {import('ipfs-core-types/src/files').ToMode} [mode] - An integer that represents the file mode\n * @property {import('ipfs-core-types/src/files').ToMTime} [mtime] - A Date object, an object with `{ secs, nsecs }` properties where secs is the number of seconds since (positive) or before (negative) the Unix Epoch began and nsecs is the number of nanoseconds since the last full second, or the output of `process.hrtime()\n * @property {boolean} [flush] - If true the changes will be immediately flushed to disk\n * @property {string} [hashAlg='sha2-256'] - The hash algorithm to use for any updated entries\n * @property {0|1} [cidVersion=0] - The CID version to use for any updated entries\n *\n * @typedef {import('../../utils').AbortOptions} AbortOptions\n */","map":null,"metadata":{},"sourceType":"script"}